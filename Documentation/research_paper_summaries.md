# Improving Factuality and Reasoning in Models through Multiagent Debate

In this paper, we present a complementary approach
to improve language responses where multiple language model instances propose
and debate their individual responses and reasoning processes over multiple rounds
to arrive at a common final answer. Our findings indicate that this approach
significantly enhances mathematical and strategic reasoning across a number of
tasks.


# Character-LLM: A Trainable Agent for Role-Playing


Overall, the paper addresses an intriguing subject of how characters can be simulated using LMs. The authors devise a novel methodology and playground for making progress on this that allows them to train agents that they then evaluate for their memories/experiences.

The paper creates a new LLM (Character LLM) which is fine tuned to behave just like a human being.

# Chain of Thought Prompting Elicits Reasoning in Large Language
Chain of thought prompting enables models to generate intermediate reasoning steps to help solve multi-step arithmetic, commonsense, and symbolic reasoning tasks.

It shows us how much simple logical analysis of LLMs can make to the output


# Gemini
Introduces Gemini as a new Mulimodal alternative. The paper claims that Gemini Ultra gives GPT4 level performance in multiple fields.
